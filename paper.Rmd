---
title: "Distributed Algorithms for Sparse Matrix-Vector Multiplication"
author: "Eric Hare"
date: "April 28, 2015"
output: pdf_document
header-includes:
    - \usepackage{float}
    - \usepackage{graphicx}
---

## Introduction
Sparse Matrix-Vector Multiplication (SpMV) is a widely used and widely explored problem. It is intrinsically parallelizable, but naive algorithms typically have very poor performance as the size of the matrix, or its sparsity drastically increases. This is because threads which handle the rows containing all or mostly all zeroes have far less work than the threads handling the dense portions of the matrix.

In this paper, I survey work that has been done to optimize the performance of these calculations. I've selected three papers which detail distributed algorithms. The three papers are:

1. Load-Balanced Sparse Matrix–Vector Multiplication on Parallel Computers by Nastea, Frieder, and El-Ghazawi
2. An architecture-aware technique for optimizing sparse matrix-vector multiplication on GPUs by Maggioni and Berger-Wolf
3. Scalable Parallel Matrix Multiplication on Distributed Memory Parallel Computers by Li

This work is relevant to my research in statistics, so I will begin by highlighting a situation in which such calculations are used in a statistical framework, before proceeding to highlight the novel contributions of each paper.

## Background
Linear models in statistics are used when we wish to use several predictor variables in order to highlight a relationship with some response variable. A simple linear model may have only a single predictor variable, but in more complex applications, there may be thousands. The predictor variables are typically represented as a design matrix $X$, where the rows correspond to the number of observations in the data, and the columns correspond to the different features. In matrix/vector notation, we can define a typical linear model in statistics as:

$y = X\beta + \epsilon$

Where:

- y is an $n \times 1$ response vector
- X is an $n \times k$ design matrix
- $\beta$ is a $k \times 1$ parameter vector
- $\epsilon$ is an $n \times 1$ error vector (that is, $\epsilon \sim MVN(0, \Sigma)$)

We wish to select estimates for $\beta$ that minimize the squared error of this function, and in doing so, obtain the result:

$b = (X^TX)^{-1}X^Ty$

Suppose we have the data given in Table \ref{tbl:cty}:
```{r, echo=FALSE, results='asis'}
library(ggplot2)
library(xtable)

print(xtable(mpg[1:5,c("cty", "hwy")], digits = 0, label = "tbl:cty", caption = "An example of five rows from a dataset containing city and highway miles per gallon."), comment = FALSE)
```

We wish to use the highway mpg to predict the city mpg. Note that typically in linear models we also include an intercept term, or a column of 1s in the design matrix $X$. Then we have:

\begin{align*}
y &= \begin{bmatrix}
        18 & 21 & 20 & 21 & 16
     \end{bmatrix}^{T} \\ \\
X &= \begin{bmatrix}
        1 & 29 \\
        1 & 29 \\
        1 & 31 \\
        1 & 30 \\
        1 & 26
     \end{bmatrix}  \\ \\
\beta &= \begin{bmatrix}
        \beta_0 \\
        \beta_1
     \end{bmatrix} \\ \\
\epsilon &= \begin{bmatrix}
        \epsilon_1 & \epsilon_2 & \epsilon_3 & \epsilon_4 & \epsilon_5
     \end{bmatrix}^{T} \\ \\
(X^TX)^{-1} &= \begin{bmatrix}
        60.2714 & -2.0714 \\
        -2.0714 & 0.0714 \\
     \end{bmatrix} \\ \\
X^Ty &= \begin{bmatrix}
        96 \\
        2797 \\
     \end{bmatrix} \\ \\
b &= (X^TX)^{-1}X^Ty = \begin{bmatrix}
        -7.7286 \\
        0.9286 \\
     \end{bmatrix}
\end{align*}

Hence, the equation for the line of best fit is given by $y = -7.7286 + 0.9286x$. Note that to derive the least squares estimators of the parameter vector $\beta$, we needed to compute the quantity $X^Ty$. In this simple example, such a computation is trivial. But in a typical setting, there may be thousands of columns of $X$ corresponding to thousands of different parameters in the parameter vector. It is not uncommon that $X$ might be very sparse. For example, in feature selection applications, there may be features corresponding to a couple of observations, when the total size of the dataset is many orders of magnitude larger. The design matrix would contain 0s for each feature not present at a particular observation.

To help illustrate the increasing complexity, consider the following block of code, which reads a dataset consisting of fitness data derived from wearable computers. The dataset consists of 4024 observations of 159 variables.

```{r, warning=FALSE}
wearables <- read.csv("data/Example_WearableComputing.csv")
X <- apply(wearables[,-1], 2, as.numeric)
y <- as.numeric(wearables[,1])

system.time(t(X) %*% y)
```

But now note what occurs when the number of observations is a replicated to be 50 times larger.

```{r, warning=FALSE}
biggerX <- do.call("rbind", replicate(50, X, simplify = FALSE))
biggery <- rep(y, times = 50)

system.time(t(biggerX) %*% biggery)
```

There are several limitations of how this calculation is implemented in R. First, there is no explicit parallelism, only thread-level parallelism implemented by the OpenMP libraries. Second, although R supports parallel processing defined explicitly, even if such an algorithm was constructed, there would be no load balancing or inter-processor communication, which makes such an algorithm far less useful for the sparse matrix case. Third, the parallel framework within R allows a choice between a multi-core (single processor) or a multi-machine (single-core) parallelism, and doesn't exploit both. Finally, because of the sparsity, there is poor cache locality related to the accesses of the elements.

## Load Balanced Sparse Matrix-Multiplication

### Overview of Algorithm
I elected to begin my survey of the literature by discussing one of the earlier proposed solutions to this problem. Load-Balanced Sparse Matrix–Vector Multiplication on Parallel Computers by Nastea, Frieder, and El-Ghazawi explores a greedy allocation algorithm for sparse pattern matrices. The paper begins by laying out a set of assumptions for the calculations:

- $Y_i = AX_i$ where A is the sparse matrix, and $X_i$ is a sequence of dense vectors
- The size of the sequence of $X_i$ vectors is very large and not a priori known
- The resulting $Y_i$ vectors are generated and transmitted on an individual basis

The fundamental aspect to the algorithm is to average the load distributed to each processor. In particular, define the following quantities:

\vspace{1cm}

$F = max_i\{\sum_{j=1}^M(nZ_{i_j})\}$

$i = 1, 2, ..., P$ represents the processors.

$i_j = \{i_1, i_2, ..., i_M\}$ represents indices of rows assigned to processor i

$nZ_{i_j}$ is the number of non-zero elements in these rows.

\vspace{1cm}

Minimizing the function $F$ amounts to minimizing the maximum number of non-zero elements assigned to a processor, and hence amounts to minimizing the largest computation time. Minimizing this function averages out the load distributed to each processor. Note that this assumes that the algorithm knows a priori the number of non-zero elements in each row. As discussed in the paper, the overhead necessary to compute this is minimal relative to the gains the the algorithm provides.

One further optimization discussed is the idea of row-splitting. If a matrix is highly skewed as well as highly sparse (that is, the non-zero elements tend to occur in blocks rather than in an even distribution throughout the matrix), significant gains can be realized by splitting the row and assigning each split to a different processor. Note that this incurs some additional overhead as a new set of indices must be kept track of. Because of this overhead, the authors recommend that this only be done in the most extreme cases of sparsity and skewness.

The full pseudo-code of the algorithm is reproduced in Figure \ref{fig:alg1}.

\begin{figure}[H]

\includegraphics[width=\linewidth]{images/alg1.png}

\caption{Pseudo-code for the greedy algorithm of the first paper. The algorithm performs operations on a major node (a leader). The leader uses the GALA routine to assign the most dense remaining rows to the processors with the minimum current workload.}
\label{fig:alg1}
\end{figure}

### Results
The performance improvements were evaluated using four different matrices of slightly different characteristics, and performing the simulation on an Intel Paragon supercomputer. Figure \ref{fig:im1} gives a table describing the different properties of the test matrices. Note the sparsity value is given as the proportion of non-zero elements. Hence, all of the matrices test are quite sparse, containing at most 5.55% non-zero elements.

\begin{figure}[H]

\includegraphics[width=\linewidth]{images/im1.png}

\caption{Matrices used for the evaluation of the algorithm. In particular, note that the last matrix is a synthetically generated matrix from a Zipf distribution. This distribution simulates sparsity and skewness characteristics that GALA is most effective at handling.}
\label{fig:im1}
\end{figure}

Four different algorithms were compared. The four algorithms are:

1. **Block** - In this allocation, no attempt at any load balancing is done. Each processor is given a contiguous block of rows for processing. In the paper, this is called the unbalanced case. Although it is balanced in terms of number of rows, it is unbalanced in terms of workload given any sort of sparsity or uneven distribution of non-zero elements.
2. **Cyclic** - Each row i is allocated to processor i mod P, where P is the number of processors. This is considered by the authors to be an example of naive load balancing, because it rests on the assumption that the distribution of non-zero elements maintains a continuous pattern throughout the matrix.
3. **Aliaga** - An iterative load balancing algorithm that generates swaps of matrix rows among processors to gradually smooth the maxima and minima of load. This algorithm has a time complexity that depends on the distribution of the data.
4. **GALA** - The algorithm presented in this paper.

The speedup as a function of the number of processors is presented in Figure \ref{fig:im2}. The top left box corresponds to the first matrix tested. Note that all but the block algorithm perform well in handling a sparse unsymmetric matrix, assuming that the distribution is not too skewed. The unsymmetric mostly block diagonal matrix has the best speedup as a function of the number of processors for both the Gala and the Aliaga algorithms, likely due to its relative density compared to the other three. The best results for the Gala algorithm come in the scenario of the fourth matrix, in which the Aliaga algorithm asymptotes at about a 12x speedup regardless of whether 20 or 40 processors are used. The Gala algorithm, meanwhile, has about a 22x speedup at 40 processors. This illustrates the success of the algorithm in handling skewness in the data distribution.

\begin{figure}[H]

\includegraphics[width=\linewidth]{images/im2.png}

\caption{Graph of the speedup of the algorithm as a function of the number of processors. Note that GALA performs most strongly on the bottom right panel, which is the panel corresponding to the synthetically generated data.}
\label{fig:im2}
\end{figure}

## Architecture Aware Technique

### Overview of Algorithm
The next paper I explored was An architecture-aware technique for optimizing sparse
matrix-vector multiplication on GPUs by Maggioni and Berger-Wolf. In this paper, a novel heuristic for reducing the number of cache accesses within hardware level thread blocks is given. They also present an improved variation of a sparse matrix data structure.

To understand the improvements, its important to note the fundamental improvements brought to the issue by the architecture of modern GPUs. A GPU is composed of several Streaming Multiprocessors (SMs), each one containing CUDA cores. In the Nvidia GTX 580, for example, there are a total of 512 cores, allowing for an optimal 512 operations per clock cycle. Each of these cores is connected to a random access memory through a cache hierarchy with two levels. Taking into account the number of threads that can theoretically be ran by a single core, GPUs offer the potential for a massive speedup through significant parallelism. But, for many of the reasons previously mentioned, these gains are not often realized when performing linear algebra tasks involving sparse matrices.

The first optimization discussed is a compression format for sparse matrices. In ELL compression, an nxm matrix is stored in an nxk data structure, where k is the maximum number of non-zero entries in any particular row. There is a separate nxk data structure storing the column index of the particular non-zero entry. Figure \ref{fig:img3} illustrates an example of using sliced ELL compression to store a sparse matrix. Sliced ELL compression is similar to regular ELL compression, except that the matrix is partitioned into different slices where there is a local value of k for each slice. This has the effect of reducing the amount of zero-padding needed in order to align the nonzero entries in the matrix.

\begin{figure}[H]

\includegraphics[width=\linewidth]{images/im3.png}

\caption{Sliced ELL compression format for sparse matrices presented in this paper. In particular, since there is a local value for the number of columns (k) for each warp, the sliced ELL format allows smaller matrices to be divided up to each warp locally.}
\label{fig:im3}
\end{figure}

The key optimization discussed in this paper surrounds improving the performance of the cache by reducing cache misses and minimizing the number of transactions that must take place. The **CACHE TRANSACTION MINIMIZATION PROBLEM** is defined formually in the paper. Given the following:

- A warp consisting of w threads (or rows)
- A list of cache line mappings $C_i$, for each thread $t_i$, corresponding to k memory elements
- A wxk scheduling table S where each row i can be any permutation of the $C_i$.

The object is to minimize $z(S) = \sum_{j=0}^{k-1}|\cap_{i=0}^{w-1} S_{i,j}|$. In other words, the goal is to minimize the sum of cache line intersections in the scheduling table S (as these are redundant transactions). The algorithm presented does so by greedily choosing a row which maximizes the amount of overlap with rows that have already been placed, which in turn will minimize the number of cache transactions. The time complexity amoritized is approximately $O(n)$ for this algorithm (technically, it is $O(k^2n)$, but because the sliced ELL format allows k to be significantly smaller in sparse matrices than n, this is approximated well by $O(n)$). In Figure \ref{fig:im5}, the results of the algorithm are given in a table. The authors note that the speedup is not significant, but that novel contributions in this arena have not performed significantly better.

\begin{figure}[H]
\centering
\includegraphics[width=0.75\linewidth]{images/im5.png}

\caption{Table listing the reduction in the number of cache transactions for several different benchmarks. Overall, a 1.09x speedup is noted by using this algorithm.}
\label{fig:im5}
\end{figure}

## Scalable Parallel Matrix Multiplication
A more generalized computation is a standard matrix multplication. This generalizes matrix-vector multiplication, and is also used in its own right in statistical computations relating to linear models. In the paper Scalable Parallel Matrix Multiplication on Distributed Memory Parallel Computers, Li presents a framework for scalable matrix multiplication that unifies theores of both sequential and parallel matrix multiplication algorithms. 

The fundmental theorem shown is as follows. For any $O(N^{\alpha})$ sequential matrix multiplication algorithm over an arbitrary ring with $2 < \alpha \le 3$, there is a fully scalable parallel implementation on Distributed Memory Parallel Computers (DMPC). That is, for all $1 \le p \le N^{\alpha}/log(N)$, multiplying two $N \times N$ matrices can be performed by a DMPC with p processors in $O(N^{\alpha}/p)$ time, and linear speedup can be achieved in the range $[1..N^{\alpha}/log(N)]$. In particular, multiplying two $N \times N$ matrices can be performed in $O(log(N))$ time by a DMPC with $N^{\alpha}/log(N)$ processors. The authors note that this matches the performance of parallel random access memory (PRAM).

DMPC has the following characteristics. Each processor has local memory, but there is no global shared memory. All processor communication is done via message passing. In a clock cycle, a processor takes either a communication or a computation step where "idle" is a valid computation step. The time complxity of DMPC, therefore, is the total number of communication and computation steps. Figure \ref{fig:dmpc} illustrates a quick diagram of a DMPC with P processors.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{images/dmpc.png}

\caption{A diagram of Distributed Memory Parallel Computers (DMPC). Note that this structure allows for one-to-one processor communication.}
\label{fig:dmpc}
\end{figure}

The authors show two lemmas which illustrate the communication capabilities. The first is that a processor group, a set of processors with consecutive indices, can send a matrix to a same sized set of processors in a single step. This of course follows from the one-to-one communications protocol of the DMPC. Lemma 2 follows naturally, which says that all processors can receive this matrix in at most log(R) + 1 steps.

The authors procede by unrolling the **recursive billinear** algorithm for performing the matrix multiplication. This algorithm is shown in Figure \ref{fig:bil}. Essentially, the algorithm performs a divide and conquer approach where the pieces of the matrix are divided recursively, and then combined into the final result. Step D (the divide step) and Step C (the combine step) in particular are highlighted as for how to perform the algorithm in parallel. The basic idea is that the equations given in the paper define a set of processors that need the results of particular computations. Using the communication system, the results are made available to the processors needing them, and each processor in parallel works on its piece of the computation.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\linewidth]{images/bil.png}

\caption{The recursive billinear algorithm for matrix multiplication. The algorithm presented in the paper unrolls this algorithm into a parallel iterative algorithm.}
\label{fig:bil}
\end{figure}

Overall, while dense, this paper provides an example of optimal parallelizability. In $O(log(N))$ time, an $N \times N$ matrix can be multiplied given $N^{\alpha}/log(N)$ where $O(N^{\alpha})$ is the running time of the sequential algorithm (which, in this paper, was the recursive billinear algorithm). 

## Conclusion
Ultimately, I felt these three papers presented a broad but interesting overview of parallel and distributed algorithms relevant to my research in Statistics. The GALA procedure yielded solid speedups for SpMV, particularly in the midst of highly sparse and high skewed data distributions. The cache optimization in the second paper yielded a relatively modest improvement. Nonetheless, the sparse matrix storage format presented could be helpful for a wide range of these applications. Lastly, the theoretical results presented in the last paper provided a solid foundation for the possibilities of parallel matrix multiplication across a set of processors. 
